{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lstm_model.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"p6TKU1y-gOEs"},"source":["# Language Modelling with Bidirectional multilayer LSTM\n","\n","In this notebook we are gonna build a Bidirectional multilayer LSTM language model.\n","\n","We handle data preparation/batching with Torchtext.\n","We use mosestokenizer for tokenizing paragraphs."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vAqj5jubpSz5","executionInfo":{"status":"ok","timestamp":1610985390004,"user_tz":-240,"elapsed":165323,"user":{"displayName":"demetre uridia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjzuY0Yge9FG8VDxYa6WIfd96riR4E8FhreaFNm=s64","userId":"07521159690453330561"}},"outputId":"6c9be99b-6dca-4a6e-9036-3b0e5ab52ca3"},"source":["!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html\r\n","!pip install -U torchtext\r\n","!pip install -U mosestokenizer"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Collecting torch==1.7.1+cu101\n","\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.7.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (735.4MB)\n","\u001b[K     |████████████████████████████████| 735.4MB 24kB/s \n","\u001b[?25hCollecting torchvision==0.8.2+cu101\n","\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.8.2%2Bcu101-cp36-cp36m-linux_x86_64.whl (12.8MB)\n","\u001b[K     |████████████████████████████████| 12.8MB 252kB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (0.8)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (3.7.4.3)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.8.2+cu101) (7.0.0)\n","Installing collected packages: torch, torchvision\n","  Found existing installation: torch 1.7.0+cu101\n","    Uninstalling torch-1.7.0+cu101:\n","      Successfully uninstalled torch-1.7.0+cu101\n","  Found existing installation: torchvision 0.8.1+cu101\n","    Uninstalling torchvision-0.8.1+cu101:\n","      Successfully uninstalled torchvision-0.8.1+cu101\n","Successfully installed torch-1.7.1+cu101 torchvision-0.8.2+cu101\n","Collecting torchtext\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/81/be2d72b1ea641afc74557574650a5b421134198de9f68f483ab10d515dca/torchtext-0.8.1-cp36-cp36m-manylinux1_x86_64.whl (7.0MB)\n","\u001b[K     |████████████████████████████████| 7.0MB 12.4MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.19.5)\n","Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.41.1)\n","Requirement already satisfied, skipping upgrade: torch==1.7.1 in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.7.1+cu101)\n","Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.23.0)\n","Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1->torchtext) (0.8)\n","Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1->torchtext) (3.7.4.3)\n","Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n","Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.10)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2020.12.5)\n","Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n","Installing collected packages: torchtext\n","  Found existing installation: torchtext 0.3.1\n","    Uninstalling torchtext-0.3.1:\n","      Successfully uninstalled torchtext-0.3.1\n","Successfully installed torchtext-0.8.1\n","Collecting mosestokenizer\n","  Downloading https://files.pythonhosted.org/packages/4b/b3/c0af235b16c4f44a2828ef017f7947d1262b2646e440f85c6a2ff26a8c6f/mosestokenizer-1.1.0.tar.gz\n","Requirement already satisfied, skipping upgrade: docopt in /usr/local/lib/python3.6/dist-packages (from mosestokenizer) (0.6.2)\n","Collecting openfile\n","  Downloading https://files.pythonhosted.org/packages/93/e6/805db6867faacb488b44ba8e0829ef4de151dd0499f3c5da5f4ad11698a7/openfile-0.0.7-py3-none-any.whl\n","Collecting uctools\n","  Downloading https://files.pythonhosted.org/packages/04/cb/70ed842d9a43460eedaa11f7503b4ab6537b43b63f0d854d59d8e150fac1/uctools-1.3.0.tar.gz\n","Collecting toolwrapper\n","  Downloading https://files.pythonhosted.org/packages/41/7b/34bf8fb69426d8a18bcc61081e9d126f4fcd41c3c832072bef39af1602cd/toolwrapper-2.1.0.tar.gz\n","Building wheels for collected packages: mosestokenizer, uctools, toolwrapper\n","  Building wheel for mosestokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mosestokenizer: filename=mosestokenizer-1.1.0-cp36-none-any.whl size=49120 sha256=54a496fb7360406f02167fb4bc75038cc10b46211e2fdfc03bde578248cb880b\n","  Stored in directory: /root/.cache/pip/wheels/a2/e7/48/48d5e0f9c0cd5def2dfd7cb8543945f906448ed1313de24a29\n","  Building wheel for uctools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for uctools: filename=uctools-1.3.0-cp36-none-any.whl size=6163 sha256=07df372cb415d7c989c439bb06829934a4c9e2538305a8346d488c23079e319e\n","  Stored in directory: /root/.cache/pip/wheels/06/b6/8f/935d5bf5bca85d47c6f5ec31641879bba057d336ab36b1e773\n","  Building wheel for toolwrapper (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for toolwrapper: filename=toolwrapper-2.1.0-cp36-none-any.whl size=3356 sha256=af230dc2fd2803593762c9d6aeada7b6c5064cbc4a05a74a0df91365c2ee639d\n","  Stored in directory: /root/.cache/pip/wheels/84/ea/29/e02f3b855bf19344972092873a1091b329309bbc3d3d0cbaef\n","Successfully built mosestokenizer uctools toolwrapper\n","Installing collected packages: openfile, uctools, toolwrapper, mosestokenizer\n","Successfully installed mosestokenizer-1.1.0 openfile-0.0.7 toolwrapper-2.1.0 uctools-1.3.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Up1tBXWuxk-K"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7qmb8gjmzWk8"},"source":["#import utils\n","import sys\n","sys.path.append('/content/drive/MyDrive/demetre_{pipia, uridia}')\n","import utils"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"393JW7k8puG7","executionInfo":{"status":"ok","timestamp":1610972104417,"user_tz":-240,"elapsed":184274,"user":{"displayName":"demetre uridia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjzuY0Yge9FG8VDxYa6WIfd96riR4E8FhreaFNm=s64","userId":"07521159690453330561"}},"outputId":"7de062d5-4964-49e8-e392-36014f05c723"},"source":["import warnings\r\n","warnings.simplefilter(action='ignore', category=FutureWarning)\r\n","warnings.simplefilter(action='ignore', category=UserWarning)\r\n","\r\n","import numpy as np\r\n","import matplotlib.pyplot as plt\r\n","%matplotlib inline\r\n","import pandas as pd\r\n","\r\n","import torch\r\n","import torch.nn as nn\r\n","import torchtext\r\n","from torchtext.datasets import PennTreebank, LanguageModelingDataset\r\n","from mosestokenizer import *\r\n","import gensim\r\n","from tqdm import tqdm_notebook\r\n","from gensim.models import KeyedVectors\r\n","import re\r\n","\r\n","# this notebook was tested with PyTorch 1.7.1 and Torchtext 0.8.1\r\n","print(torch.__version__, torchtext.__version__) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["1.7.1+cu101 0.8.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"L6ERVYNszfwO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610972104421,"user_tz":-240,"elapsed":184262,"user":{"displayName":"demetre uridia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjzuY0Yge9FG8VDxYa6WIfd96riR4E8FhreaFNm=s64","userId":"07521159690453330561"}},"outputId":"6a68579a-5750-4779-cc5c-0c0c7df99c34"},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"XRRuRn4WzaYn"},"source":["# using utils\n","# Get text field and data loaders for lstm model\n","\n","w2v_model_path = '/content/drive/MyDrive/demetre_{pipia, uridia}/resources/word2vec.model_paragraph_all_only_georgian_shuffled_3M_30it'\n","df_path = '/content/drive/MyDrive/demetre_{pipia, uridia}/data/paragraph_all_only_georgian_shuffled.csv'\n","text_field = utils.TextField(w2v_model_path, df_path) \n","txt_field = text_field.get_txt_field()\n","\n","train_dl, dev_dl, test_dl = utils.DataLoader(text_field, device).get_dls()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F9uL-RgQ7754","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610972198785,"user_tz":-240,"elapsed":277063,"user":{"displayName":"demetre uridia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjzuY0Yge9FG8VDxYa6WIfd96riR4E8FhreaFNm=s64","userId":"07521159690453330561"}},"outputId":"89a569d2-db08-4a6e-a83a-159a6b55fccb"},"source":["len(train_dl), len(dev_dl), len(test_dl)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5319, 285, 285)"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"Vnoogns516cq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610972207889,"user_tz":-240,"elapsed":285580,"user":{"displayName":"demetre uridia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjzuY0Yge9FG8VDxYa6WIfd96riR4E8FhreaFNm=s64","userId":"07521159690453330561"}},"outputId":"14f83dc1-a290-46e9-c41c-e6fdb730282c"},"source":["batch = next(iter(train_dl))\r\n","\r\n","# output shape should be (batch_size, bptt_len). Note that, if we used batch_first=False in above, we would've got (bptt_len, batch_size) instead.\r\n","batch.text.shape, batch.target.shape "],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([128, 10]), torch.Size([128, 10]))"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"sjWA5MGF2Qr_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610972207892,"user_tz":-240,"elapsed":284814,"user":{"displayName":"demetre uridia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjzuY0Yge9FG8VDxYa6WIfd96riR4E8FhreaFNm=s64","userId":"07521159690453330561"}},"outputId":"9b43c450-4155-46cc-9299-2c3111ce9aa4"},"source":["# Note that 'target' is left-shifted version of 'text', as we want for next word prediction!\r\n","batch.text[11], batch.target[11]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([    0,     0,  1614,   495, 27260, 20908,     4,    73,     0, 16530],\n","        device='cuda:0'),\n"," tensor([    0,  1614,   495, 27260, 20908,     4,    73,     0, 16530,   677],\n","        device='cuda:0'))"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"vugZvzsF2h_g"},"source":["hidden_dim = 100\r\n","model = utils.LSTMModel(utils.EMBED_SIZE, hidden_dim, len(txt_field.vocab), txt_field,device, num_layers=2).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YdDKfeOsk1JM"},"source":["# Train"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4hFEp6jOkSxM","executionInfo":{"status":"ok","timestamp":1610972879570,"user_tz":-240,"elapsed":940341,"user":{"displayName":"demetre uridia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjzuY0Yge9FG8VDxYa6WIfd96riR4E8FhreaFNm=s64","userId":"07521159690453330561"}},"outputId":"d1e92296-4795-46d7-b440-3a8e80932371"},"source":["epochs = 1\n","model_save_path = '/content/drive/MyDrive/demetre_{pipia, uridia}/resources/model_tmp'\n","utils.train_loop(model, train_dl, dev_dl, device, epochs, model_save_path)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1 | Iter 100 | Avg Train Loss 11.275244369506837 | Dev Perplexity None | LR  0.0001\n","Epoch 1 | Iter 200 | Avg Train Loss 9.500002593994141 | Dev Perplexity None | LR  0.0001\n","Epoch 1 | Iter 300 | Avg Train Loss 8.438881168365478 | Dev Perplexity None | LR  0.0001\n","Epoch 1 | Iter 400 | Avg Train Loss 8.383275566101075 | Dev Perplexity None | LR  0.0001\n","Epoch 1 | Iter 500 | Avg Train Loss 8.355646858215332 | Dev Perplexity 3854.6311255381393 | LR  0.0001\n","Epoch 1 | Iter 600 | Avg Train Loss 8.322541770935059 | Dev Perplexity 3854.6311255381393 | LR  0.0001\n","Epoch 1 | Iter 700 | Avg Train Loss 8.32110122680664 | Dev Perplexity 3854.6311255381393 | LR  0.0001\n","Epoch 1 | Iter 800 | Avg Train Loss 8.288846797943116 | Dev Perplexity 3854.6311255381393 | LR  0.0001\n","Epoch 1 | Iter 900 | Avg Train Loss 8.309331703186036 | Dev Perplexity 3854.6311255381393 | LR  0.0001\n","Epoch 1 | Iter 1000 | Avg Train Loss 8.286775550842286 | Dev Perplexity 3570.756080078349 | LR  0.0001\n","Epoch 1 | Iter 1100 | Avg Train Loss 8.265127000808716 | Dev Perplexity 3570.756080078349 | LR  0.0001\n","Epoch 1 | Iter 1200 | Avg Train Loss 8.247139177322389 | Dev Perplexity 3570.756080078349 | LR  0.0001\n","Epoch 1 | Iter 1300 | Avg Train Loss 8.212179827690125 | Dev Perplexity 3570.756080078349 | LR  0.0001\n","Epoch 1 | Iter 1400 | Avg Train Loss 8.138619327545166 | Dev Perplexity 3570.756080078349 | LR  0.0001\n","Epoch 1 | Iter 1500 | Avg Train Loss 8.085858173370362 | Dev Perplexity 2867.704333568103 | LR  0.0001\n","Epoch 1 | Iter 1600 | Avg Train Loss 7.997257061004639 | Dev Perplexity 2867.704333568103 | LR  0.0001\n","Epoch 1 | Iter 1700 | Avg Train Loss 7.962584238052369 | Dev Perplexity 2867.704333568103 | LR  0.0001\n","Epoch 1 | Iter 1800 | Avg Train Loss 7.955871639251709 | Dev Perplexity 2867.704333568103 | LR  0.0001\n","Epoch 1 | Iter 1900 | Avg Train Loss 7.851601305007935 | Dev Perplexity 2867.704333568103 | LR  0.0001\n","Epoch 1 | Iter 2000 | Avg Train Loss 7.8271627569198605 | Dev Perplexity 2153.721047085588 | LR  0.0001\n","Epoch 1 | Iter 2100 | Avg Train Loss 7.773612451553345 | Dev Perplexity 2153.721047085588 | LR  0.0001\n","Epoch 1 | Iter 2200 | Avg Train Loss 7.7226762866973875 | Dev Perplexity 2153.721047085588 | LR  0.0001\n","Epoch 1 | Iter 2300 | Avg Train Loss 7.668889293670654 | Dev Perplexity 2153.721047085588 | LR  0.0001\n","Epoch 1 | Iter 2400 | Avg Train Loss 7.621473407745361 | Dev Perplexity 2153.721047085588 | LR  0.0001\n","Epoch 1 | Iter 2500 | Avg Train Loss 7.554221043586731 | Dev Perplexity 1614.182702491527 | LR  0.0001\n","Epoch 1 | Iter 2600 | Avg Train Loss 7.503116803169251 | Dev Perplexity 1614.182702491527 | LR  0.0001\n","Epoch 1 | Iter 2700 | Avg Train Loss 7.45805655002594 | Dev Perplexity 1614.182702491527 | LR  0.0001\n","Epoch 1 | Iter 2800 | Avg Train Loss 7.405841412544251 | Dev Perplexity 1614.182702491527 | LR  0.0001\n","Epoch 1 | Iter 2900 | Avg Train Loss 7.339753313064575 | Dev Perplexity 1614.182702491527 | LR  0.0001\n","Epoch 1 | Iter 3000 | Avg Train Loss 7.315270457267761 | Dev Perplexity 1273.8277637174115 | LR  0.0001\n","Epoch 1 | Iter 3100 | Avg Train Loss 7.255060949325562 | Dev Perplexity 1273.8277637174115 | LR  0.0001\n","Epoch 1 | Iter 3200 | Avg Train Loss 7.203875608444214 | Dev Perplexity 1273.8277637174115 | LR  0.0001\n","Epoch 1 | Iter 3300 | Avg Train Loss 7.2151806974411015 | Dev Perplexity 1273.8277637174115 | LR  0.0001\n","Epoch 1 | Iter 3400 | Avg Train Loss 7.163526287078858 | Dev Perplexity 1273.8277637174115 | LR  0.0001\n","Epoch 1 | Iter 3500 | Avg Train Loss 7.111357426643371 | Dev Perplexity 1025.5624201732044 | LR  0.0001\n","Epoch 1 | Iter 3600 | Avg Train Loss 7.084699134826661 | Dev Perplexity 1025.5624201732044 | LR  0.0001\n","Epoch 1 | Iter 3700 | Avg Train Loss 7.01839614868164 | Dev Perplexity 1025.5624201732044 | LR  0.0001\n","Epoch 1 | Iter 3800 | Avg Train Loss 6.962406721115112 | Dev Perplexity 1025.5624201732044 | LR  0.0001\n","Epoch 1 | Iter 3900 | Avg Train Loss 6.949412050247193 | Dev Perplexity 1025.5624201732044 | LR  0.0001\n","Epoch 1 | Iter 4000 | Avg Train Loss 6.921997437477112 | Dev Perplexity 835.89040517877 | LR  0.0001\n","Epoch 1 | Iter 4100 | Avg Train Loss 6.879151930809021 | Dev Perplexity 835.89040517877 | LR  0.0001\n","Epoch 1 | Iter 4200 | Avg Train Loss 6.864047222137451 | Dev Perplexity 835.89040517877 | LR  0.0001\n","Epoch 1 | Iter 4300 | Avg Train Loss 6.790662612915039 | Dev Perplexity 835.89040517877 | LR  0.0001\n","Epoch 1 | Iter 4400 | Avg Train Loss 6.736819176673889 | Dev Perplexity 835.89040517877 | LR  0.0001\n","Epoch 1 | Iter 4500 | Avg Train Loss 6.7149734830856325 | Dev Perplexity 690.4620950572839 | LR  0.0001\n","Epoch 1 | Iter 4600 | Avg Train Loss 6.673553900718689 | Dev Perplexity 690.4620950572839 | LR  0.0001\n","Epoch 1 | Iter 4700 | Avg Train Loss 6.638105492591858 | Dev Perplexity 690.4620950572839 | LR  0.0001\n","Epoch 1 | Iter 4800 | Avg Train Loss 6.6569441270828245 | Dev Perplexity 690.4620950572839 | LR  0.0001\n","Epoch 1 | Iter 4900 | Avg Train Loss 6.578670029640198 | Dev Perplexity 690.4620950572839 | LR  0.0001\n","Epoch 1 | Iter 5000 | Avg Train Loss 6.589301881790161 | Dev Perplexity 578.6875094702347 | LR  0.0001\n","Epoch 1 | Iter 5100 | Avg Train Loss 6.517085032463074 | Dev Perplexity 578.6875094702347 | LR  0.0001\n","Epoch 1 | Iter 5200 | Avg Train Loss 6.508386235237122 | Dev Perplexity 578.6875094702347 | LR  0.0001\n","Epoch 1 | Iter 5300 | Avg Train Loss 6.493413972854614 | Dev Perplexity 578.6875094702347 | LR  0.0001\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"d80d9ftL2T_3"},"source":["# Count perplexity on Test data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OUO2DDj1mxjF","executionInfo":{"status":"ok","timestamp":1610973007016,"user_tz":-240,"elapsed":29110,"user":{"displayName":"demetre uridia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjzuY0Yge9FG8VDxYa6WIfd96riR4E8FhreaFNm=s64","userId":"07521159690453330561"}},"outputId":"5ccc2724-3cfb-436f-ccdc-e629e5d6151a"},"source":["utils.compute_perplexity(model, test_dl, device)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["520.5526483478595"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"_2LxAvfD6XVc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610895723139,"user_tz":-240,"elapsed":8176,"user":{"displayName":"demetre pipia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4CbIRo9dennmXZMhW6FoV-wXK_9w-s1pDeMNv=s64","userId":"11756928928575830424"}},"outputId":"83ad4050-a3e9-43ec-e1eb-d4e43c872c75"},"source":["#load model\r\n","model = utils.LSTMModel(utils.EMBED_SIZE, 100, len(txt_field.vocab), txt_field,device, num_layers=2).to(device)\r\n","model.load_state_dict(torch.load('/content/drive/MyDrive/demetre_{pipia, uridia}/resources/lstm_model_300K_tuning', map_location=device))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"c9T388b67cWB"},"source":["# Save embeddings after train"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a881_2DV3inA","executionInfo":{"status":"ok","timestamp":1610896787092,"user_tz":-240,"elapsed":715,"user":{"displayName":"demetre pipia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4CbIRo9dennmXZMhW6FoV-wXK_9w-s1pDeMNv=s64","userId":"11756928928575830424"}},"outputId":"d7a512f9-700c-4a80-aab2-088d9a654941"},"source":["embs = model.state_dict()['emb.weight']\r\n","embs.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([87151, 100])"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"EBoISKqg0-85"},"source":["with open(\"/content/drive/MyDrive/demetre_{pipia, uridia}/resources/word2vec_from_trained_model\", 'a') as the_file:\r\n","    the_file.write(str(embs.size(0)) + ' 100')\r\n","    the_file.write('\\n')\r\n","    for i in range(embs.size(0)):\r\n","        the_file.write(txt_field.vocab.itos[i] + ' ' + ' '.join([str(emb.item()) for emb in embs[i]]))\r\n","        the_file.write('\\n')"],"execution_count":null,"outputs":[]}]}